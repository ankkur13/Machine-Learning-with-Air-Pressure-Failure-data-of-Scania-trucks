{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "                  LR_Model\n",
      "Metrics_Train      0.94397\n",
      "Metrics_Test       0.93257\n",
      "Cost           21740.00000\n",
      "Random Forest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ankur\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "C:\\Users\\Ankur\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "C:\\Users\\Ankur\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  LR_Model     RF_Model\n",
      "Metrics_Train      0.94397      0.94586\n",
      "Metrics_Test       0.93257      0.90957\n",
      "Cost           21740.00000  28760.00000\n",
      "Support Vector Machine\n",
      "                  LR_Model     RF_Model    SVM_Model\n",
      "Metrics_Train      0.94397      0.94586      0.84221\n",
      "Metrics_Test       0.93257      0.90957      0.90548\n",
      "Cost           21740.00000  28760.00000  30120.00000\n",
      "XG Boost\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ankur\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\Users\\Ankur\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  LR_Model     RF_Model    SVM_Model    XGB_Model\n",
      "Metrics_Train      0.94397      0.94586      0.84221      0.94417\n",
      "Metrics_Test       0.93257      0.90957      0.90548      0.93475\n",
      "Cost           21740.00000  28760.00000  30120.00000  21640.00000\n",
      "Metrics_Score file uploaded\n",
      "Pickle file uploaded \n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import KFold,cross_val_score\n",
    "from sklearn.metrics import accuracy_score,roc_curve,confusion_matrix,precision_recall_curve,auc,roc_auc_score,recall_score,classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from collections import OrderedDict\n",
    "from sklearn import svm\n",
    "import random\n",
    "from sklearn.model_selection import KFold\n",
    "import missingno as msno\n",
    "from IPython.display import display, Markdown\n",
    "import pickle\n",
    "from sklearn import metrics\n",
    "from pandas import Series\n",
    "from collections import defaultdict\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "def main_function():\n",
    "    \n",
    "    #cik = sys.argv[1]\n",
    "    #accession = sys.argv[2]\n",
    "\n",
    "    data_ingestion() \n",
    "       \n",
    "    #close_program(sys.argv[3], sys.argv[4])\n",
    "\n",
    "def data_ingestion():\n",
    "    df=pd.read_csv('aps_failure_training_set.csv',error_bad_lines=False)\n",
    "    df_test=pd.read_csv('aps_failure_test_set.csv',error_bad_lines=False)\n",
    "    data_manipulation(df,df_test)\n",
    "\n",
    "def data_manipulation(df,df_test):\n",
    "    #Training Data Manipulation\n",
    "    df = df.rename(columns = {'class' : 'Flag'})\n",
    "    df['Flag'] = df.Flag.map({'neg':0, 'pos':1})\n",
    "    df = df.replace(['na'],np.nan)\n",
    "    df_X = df.loc[:,df.columns != 'Flag']\n",
    "    df_Y = df.loc[:,df.columns == 'Flag']\n",
    "    df_X = df_X.apply(pd.to_numeric)\n",
    "    df_X= df_X.fillna(df_X.median()).dropna(axis =1 , how ='all')\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(df_X)\n",
    "    df_X = scaler.transform(df_X)\n",
    "    pca = PCA(0.95)\n",
    "    pca.fit(df_X)\n",
    "    pca_n=pca.n_components_\n",
    "    df_X = pca.transform(df_X)\n",
    "    df_X= pd.DataFrame(df_X)\n",
    "    \n",
    "    #Testing Data Manipulation\n",
    "    df_test = df_test.rename(columns = {'class' : 'Flag'})\n",
    "    df_test = df_test.replace(['na'],[np.NaN])\n",
    "    df_test['Flag'] = df_test.Flag.map({'neg':0, 'pos':1})\n",
    "    df_test_X = df_test.loc[:,df_test.columns != 'Flag']\n",
    "    df_test_Y = df_test.loc[:,df_test.columns == 'Flag']\n",
    "    df_test_X = df_test_X.apply(pd.to_numeric)\n",
    "    df_test_X= df_test_X.fillna(df_test_X.median()).dropna(axis =1 , how ='all')\n",
    "    scaler.fit(df_test_X)\n",
    "    df_test_X = scaler.transform(df_test_X)\n",
    "    pca = PCA(pca_n)\n",
    "    pca.fit(df_test_X)\n",
    "    pca.n_components_\n",
    "    df_test_X = pca.transform(df_test_X)\n",
    "    df_test_X= pd.DataFrame(df_test_X)\n",
    "    \n",
    "    X_train,X_validation,Y_train,Y_validation = train_test_split(df_X,df_Y,test_size = 0.2,random_state = 0)\n",
    "    DF = pd.concat([X_train,Y_train],axis = 1)\n",
    "    fun_undersampling(DF,X_validation,Y_validation,df_test_X,df_test_Y)\n",
    "\n",
    "def fun_undersampling(DF,X_validation,Y_validation,df_test_X,df_test_Y):\n",
    "\n",
    "    numberofrecords_pos = len(DF[DF.Flag == 1])\n",
    "    pos_indices = np.array(DF[DF.Flag == 1].index)\n",
    "    #Picking the indices of the normal class\n",
    "    neg_indices = DF[DF.Flag == 0].index\n",
    "    #out of indices selected, randomly select \"x\" number of records\n",
    "    random_neg_indices = np.random.choice(neg_indices, numberofrecords_pos, replace = False)\n",
    "    random_neg_indices =np.array(random_neg_indices)\n",
    "    #Appending the two indices\n",
    "    under_sample_indices = np.concatenate([pos_indices,random_neg_indices])\n",
    "    #Undersample dataset\n",
    "    under_sample_data = DF.loc[under_sample_indices,:]\n",
    "    X_undersample = under_sample_data.loc[:,under_sample_data.columns != 'Flag']\n",
    "    Y_undersample = under_sample_data.loc[:,under_sample_data.columns == 'Flag']\n",
    "    \n",
    "    fun_MachineLeaarningModels(X_undersample,Y_undersample,X_validation,Y_validation,df_test_X,df_test_Y)\n",
    "    \n",
    "    \n",
    "\n",
    "# Function to print and add metrics to dataframe\n",
    "def print_metrics(df, model, recall_train,recall_test,cost):\n",
    "    df[model] = [float(\"{0:.5f}\".format(recall_train)), float(\"{0:.5f}\".format(recall_test)),cost]\n",
    "    return df\n",
    "\n",
    "    # Function to print and add metrics to dataframe\n",
    "def print_metrics_copy(df1, model, recall_train,recall_test,cost,model_var):\n",
    "    df1[model] = [float(\"{0:.5f}\".format(recall_train)), float(\"{0:.5f}\".format(recall_test)),cost,model_var]\n",
    "    return df1\n",
    "        \n",
    "def fun_MachineLeaarningModels(X_undersample,Y_undersample,X_validation,Y_validation,df_test_X,df_test_Y):\n",
    "\n",
    "    metrics_df = pd.DataFrame(index = ['Metrics_Train','Metrics_Test','Cost'])\n",
    "    metrics_df_copy = pd.DataFrame(index = ['Metrics_Train','Metrics_Test','Cost','Model_Var'])\n",
    "    \n",
    "    #Logistic Regression\n",
    "    print('Logistic Regression')\n",
    "    lr_train = LogisticRegression(C =0.1,penalty = 'l2')\n",
    "    lr_train.fit(X_undersample,Y_undersample.values.ravel())\n",
    "    y_pred_train = lr_train.predict(X_validation)\n",
    "    recall_train_lr = roc_auc_score(Y_validation,y_pred_train)\n",
    "    \n",
    "    lr_test = LogisticRegression(C =0.001,penalty = 'l2')\n",
    "    lr_fit=lr_test.fit(X_undersample,Y_undersample.values.ravel())\n",
    "    y_pred_test = lr_test.predict(df_test_X)\n",
    "    recall_test_lr=roc_auc_score(df_test_Y,y_pred_test)\n",
    "    \n",
    "    cm = confusion_matrix(df_test_Y,y_pred_test).ravel()\n",
    "    cm = pd.DataFrame(cm.reshape((1,4)), columns=['TN', 'FP', 'FN', 'TP'])\n",
    "    \n",
    "    total_cost = 10*cm.FP + 500*cm.FN\n",
    "    \n",
    "    metrics_df = print_metrics(metrics_df, 'LR_Model', recall_train_lr, recall_test_lr,total_cost[0])\n",
    "    metrics_df_copy = print_metrics_copy(metrics_df_copy, 'LR_Model', recall_train_lr, recall_test_lr,total_cost[0],lr_test)\n",
    "    print(metrics_df)\n",
    "    \n",
    "    #Random Forest\n",
    "    print('Random Forest')\n",
    "    clf_train = RandomForestClassifier(n_estimators=80,max_features= 'log2',oob_score =True)\n",
    "    rf_fit=clf_train.fit(X_undersample,Y_undersample.values.ravel())\n",
    "    y_pred_train_rf = clf_train.predict(X_validation)\n",
    "    recall_train_rf=roc_auc_score(Y_validation,y_pred_train_rf)\n",
    "            \n",
    "    clf_test = RandomForestClassifier(n_estimators=15,max_features= 'sqrt',oob_score =True)\n",
    "    rf_fit=clf_test.fit(X_undersample,Y_undersample.values.ravel())\n",
    "    y_pred_test_rf = clf_test.predict(df_test_X)\n",
    "    recall_test_rf=roc_auc_score(df_test_Y,y_pred_test_rf)\n",
    "    \n",
    "    cm = confusion_matrix(df_test_Y,y_pred_test_rf).ravel()\n",
    "    cm = pd.DataFrame(cm.reshape((1,4)), columns=['TN', 'FP', 'FN', 'TP'])\n",
    "    \n",
    "    total_cost = 10*cm.FP + 500*cm.FN\n",
    "    \n",
    "    metrics_df = print_metrics(metrics_df, 'RF_Model', recall_train_rf, recall_test_rf,total_cost[0])\n",
    "    metrics_df_copy = print_metrics_copy(metrics_df_copy, 'RF_Model', recall_train_rf, recall_test_rf,total_cost[0],clf_test)\n",
    "    print(metrics_df)\n",
    "    \n",
    "    #Support Vector Machine\n",
    "    print('Support Vector Machine')\n",
    "    svm_train = svm.SVC(C =0.1,gamma = 0.01, kernel = 'sigmoid')\n",
    "    svm_fit=svm_train.fit(X_undersample,Y_undersample)\n",
    "    y_pred_train_svm = svm_train.predict(X_validation)\n",
    "    recall_train_svm=roc_auc_score(Y_validation,y_pred_train_svm)\n",
    "    \n",
    "    svm_test = svm.SVC(C =0.01,gamma = 0.01, kernel = 'sigmoid')\n",
    "    svm_fit=svm_test.fit(X_undersample,Y_undersample)\n",
    "    y_pred_test_svm = svm_test.predict(df_test_X)\n",
    "    recall_test_svm=roc_auc_score(df_test_Y,y_pred_test_svm)\n",
    "    \n",
    "    cm = confusion_matrix(df_test_Y,y_pred_test_svm).ravel()\n",
    "    cm = pd.DataFrame(cm.reshape((1,4)), columns=['TN', 'FP', 'FN', 'TP'])\n",
    "    \n",
    "    total_cost = 10*cm.FP + 500*cm.FN\n",
    "    \n",
    "    metrics_df = print_metrics(metrics_df, 'SVM_Model', recall_train_svm, recall_test_svm,total_cost[0])\n",
    "    metrics_df_copy = print_metrics_copy(metrics_df_copy, 'SVM_Model', recall_train_svm, recall_test_svm,total_cost[0],svm_test)\n",
    "    print(metrics_df)\n",
    "    \n",
    "    #XG Boost\n",
    "    print('XG Boost')\n",
    "    xgb=XGBClassifier()\n",
    "    xgb_fit=xgb.fit(X_undersample,Y_undersample.values.ravel())\n",
    "    y_pred_train_xgb = xgb.predict(X_validation)\n",
    "    recall_train_xgb=roc_auc_score(Y_validation,y_pred_train_xgb)\n",
    "    \n",
    "    y_pred_test_xgb = xgb.predict(df_test_X)\n",
    "    recall_test_xgb=roc_auc_score(df_test_Y,y_pred_test_xgb)\n",
    "    \n",
    "    cm = confusion_matrix(df_test_Y,y_pred_test_xgb).ravel()\n",
    "    cm = pd.DataFrame(cm.reshape((1,4)), columns=['TN', 'FP', 'FN', 'TP'])\n",
    "    \n",
    "    total_cost = 10*cm.FP + 500*cm.FN\n",
    "    \n",
    "    # Printing the training and testing metrices\n",
    "    metrics_df = print_metrics(metrics_df, 'XGB_Model', recall_train_xgb, recall_test_xgb,total_cost[0])\n",
    "    metrics_df_copy = print_metrics_copy(metrics_df_copy, 'XGB_Model', recall_train_svm, recall_test_svm,total_cost[0],xgb)\n",
    "    print(metrics_df)\n",
    "\n",
    "    metrics_df=metrics_df.T\n",
    "    metrics_df_copy=metrics_df_copy.T\n",
    "    \n",
    "    #Ranking of the model\n",
    "    metrics_df['Model_Rank'] = metrics_df['Cost'].rank(ascending=True,method='max')\n",
    "    metrics_df['Model_Rank']=metrics_df['Model_Rank'].astype(int)\n",
    "    metrics_df['Model_Name']=['Logistic Regression','Random Forest','Support Vector Machine','XGB']\n",
    "    metrics_df_copy['Model_Rank'] = metrics_df_copy['Cost'].rank(ascending=True,method='max')\n",
    "    metrics_df_copy['Model_Rank']=metrics_df_copy['Model_Rank'].astype(int)\n",
    "    store_result_csv(metrics_df)\n",
    "    fun_pickle(metrics_df,metrics_df_copy)\n",
    "    return metrics_df\n",
    "\n",
    "def store_result_csv(metrics):\n",
    "    metrics_df=metrics\n",
    "    metrics_df.to_csv('Metrics_Score.csv', index=False)\n",
    "    print('Metrics_Score file uploaded')\n",
    "    \n",
    "def fun_pickle(metrics_df,metrics_df_copy):\n",
    "    \n",
    "    dictionary1=pd.Series(metrics_df.Model_Name.values,index=metrics_df.Model_Rank).to_dict()\n",
    "    dictionary2=pd.Series(metrics_df_copy.Model_Var.values,index=metrics_df_copy.Model_Rank).to_dict()\n",
    "    \n",
    "    \n",
    "    ds = [dictionary1, dictionary2]\n",
    "    \n",
    "    dictionary = {}\n",
    "    for k in dictionary1.keys():\n",
    "        dictionary[k] = tuple(dictionary[k] for dictionary in ds)\n",
    "    \n",
    "    filename = 'finalized_model.pkl'\n",
    "    pickle.dump(dictionary, open(filename, 'wb'))\n",
    "    print('Pickle file uploaded ')\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
